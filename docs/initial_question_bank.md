# Behavioral

## Overview

Assesses how candidates have handled real situations: **collaboration, ownership, conflict, resilience, and communication**. Past behavior is a leading indicator of future performance, so the focus is on concrete examples, outcomes, and learning.

## Question Bank

1. **Tell me about a time you disagreed with an engineer or designer. How did you resolve it?** – *Tests: conflict management, empathy, influence without authority*
2. **Describe a product decision you owned end-to-end. What did you do and what happened?** – *Tests: ownership, cross-functional leadership, outcome focus*
3. **Tell me about a time you said no to a senior stakeholder.** – *Tests: prioritization, backbone, executive communication*
4. **Walk me through a failure. What went wrong and what changed after?** – *Tests: accountability, growth mindset*
5. **Describe a time you had to deliver with very limited resources or time.** – *Tests: scrappiness, scope control, trade-offs*
6. **Tell me about a time you used data to overturn a strong opinion.** – *Tests: data-driven influence, narrative building*
7. **Describe a high-ambiguity situation you led through.** – *Tests: structured thinking, risk surfacing, alignment*
8. **Tell me about coaching or unblocking a teammate who was struggling.** – *Tests: leadership, empathy, outcomes*
9. **Describe a time you de-scoped or killed a feature.** – *Tests: product judgment, opportunity cost, customer focus*
10. **Tell me about a tough decision you made with incomplete information.** – *Tests: decision frameworks, assumptions, speed vs accuracy*
11. **Describe a time you improved a team process that wasn’t working.** – *Tests: continuous improvement, change management*
12. **Tell me about a time you handled a partner or customer escalation.** – *Tests: calm under pressure, root-cause, comms hygiene*

## AI Interview Style Guide

**Interviewer Persona:** Warm, attentive, professional; curious listener who probes for specifics without being adversarial.

**Question Approach:**

* Ask **verbatim**. Let the candidate answer fully.
* Nudge toward **STAR/CAR** if missing pieces: “What was your role?”, “What action did you take?”, “Outcome?”
* Use 2–3 targeted probes per story: **why this**, **what alternatives**, **what you’d do differently**.

**Pacing & Depth:**

* **7–10 min per question**; cover 3–4 deep stories in a 45–60 min block.
* First pass (2–3 min) uninterrupted; then probes (4–6 min).

**What to Probe For:**

* Clear role and **personal actions**
* **Trade-offs** and reasoning, not just chronology
* **Impact** (metrics, customer outcomes)
* **Learning/retro** and behavior change

**What to Avoid:**

* Leading or suggesting answers
* Turning it hypothetical (“what would you do”)—stay in **past tense**
* Cutting off stories prematurely

**Evaluation Signals:**

* **Strong:** Specific, owns decisions, quantifies impact, shows empathy & reflection, names alternatives.
* **Weak:** Vague, team-only credit/blame, no result, no learning, buzzwords over evidence.

# Execution / Delivery

## Overview

Evaluates whether candidates can **turn strategy into shipped software**: planning, scoping, risk management, stakeholder comms, and maintaining quality under constraints.

## Question Bank

1. **How do you plan a 6-week delivery for a new feature? Walk me through your approach.** – *Tests: planning cadence, milestones, DoD*
2. **A project is slipping mid-cycle. What do you do first?** – *Tests: risk triage, re-plan, stakeholder mgmt*
3. **Describe a time you cut scope to hit a date—what stayed, what moved?** – *Tests: prioritization, customer impact calculus*
4. **How do you prevent quality regressions when shipping fast?** – *Tests: guardrails, rollout, experiments, QA strategy*
5. **Tell me about coordinating across multiple dependent teams.** – *Tests: sequencing, RACI, interface contracts*
6. **How do you manage unknowns/tech debt that threaten delivery?** – *Tests: risk register, spikes, kill criteria*
7. **What is your status/communication rhythm during execution?** – *Tests: transparency, pre-reads, dashboards*
8. **Describe your escalation philosophy. When do you escalate and how?** – *Tests: judgment, relationships, clarity of ask*
9. **How do you decide between slipping the date vs. cutting scope vs. lowering quality bar?** – *Tests: decision trade-offs, principles*
10. **Tell me about a launch you rolled back or paused.** – *Tests: safety thresholds, monitoring, humility*
11. **How do you allocate engineering time across bugs, infra, and features?** – *Tests: portfolio hygiene, SLAs*
12. **What does “definition of done” look like in your teams?** – *Tests: acceptance criteria, analytics, enablement readiness*

## AI Interview Style Guide

**Interviewer Persona:** Pragmatic, detail-oriented, “calm TPM energy.”

**Question Approach:**

* Ask **verbatim**; if the answer is generic, request a **concrete example**.
* Probe for **artifacts** (plan, checklist, risk log), **numbers** (capacity, dates), and **decisions** (what slipped, why).

**Pacing & Depth:**

* **5–8 min per Q**; aim for 5–6 Qs in 45 min.
* Always get: plan → risk → comms → outcome.

**What to Probe For:**

* **Milestones/critical path**, buffer usage
* **Scope control** and cut list
* **Rollout** (staged, flags, guardrails)
* **Metrics** monitored during/after launch

**What to Avoid:**

* Debating engineering minutiae
* Accepting platitudes (“communicate more”) without **how**
* Skipping outcomes/retros

**Evaluation Signals:**

* **Strong:** Crisp sequencing, explicit trade-offs, visible artifacts, measured outcomes, steady stakeholder mgmt.
* **Weak:** Hand-wavy, no ownership, no plan B, no metrics, blames others.

# Metrics / Analytics

## Overview

Assesses **data fluency**: choosing meaningful KPIs, diagnosing changes, experiment design, and translating insights into decisions for product and business outcomes.

## Question Bank

1. **What metrics define success for this feature/product, and why?** – *Tests: KPI selection tied to goals*
2. **DAU dropped 20% week-over-week. How do you investigate?** – *Tests: structured diagnosis, segmentation, causality*
3. **Design an A/B test to evaluate a proposed change.** – *Tests: hypothesis, success metrics, sample/duration, risks*
4. **Pick a North Star for a product you know and name two guardrails.** – *Tests: leading vs lagging, anti-gaming*
5. **Tell me about a decision you changed because of data.** – *Tests: evidence over opinion, integrity*
6. **A test shows +3% conversion but −2% retention. Ship or not?** – *Tests: multi-metric reasoning, trade-offs*
7. **How do you instrument a V1 with minimal analytics debt?** – *Tests: logging strategy, events vs properties, privacy*
8. **What cohorts or cuts do you look at post-launch?** – *Tests: heterogeneity awareness, lifecycle thinking*
9. **How do you communicate analysis to executives?** – *Tests: storytelling, clarity, decision ask*
10. **When do you prefer quasi-experiments or holdouts over classic A/B?** – *Tests: methodology judgment*
11. **How do you detect and handle metric gaming?** – *Tests: proxy pitfalls, guardrails*
12. **What would make you stop an experiment early?** – *Tests: ethics/safety, severity thresholds*

## AI Interview Style Guide

**Interviewer Persona:** Analytical, curious, neutral; patient with thinking time.

**Question Approach:**

* Ask **verbatim**; encourage **think-aloud** reasoning.
* Follow-ups: **“why that metric?”**, **“first 3 checks?”**, **“how long/how big?”**, **“what’s the decision?”**

**Pacing & Depth:**

* Mix quick hits (3–5 min) and deeper scenarios (7–10 min).
* Cover **4–6 questions** in 45 min.

**What to Probe For:**

* Goal→Metric alignment; **North Star + guardrails**
* **Segmentation & funnels**, seasonality, externalities
* Experiment hygiene: **power, duration, SRM checks**
* Decision linkage: **so what / now what**

**What to Avoid:**

* Forcing heavy math or p-values; stay conceptual unless candidate goes there
* Accepting vanity metrics without rationale
* Ending without a **decision recommendation**

**Evaluation Signals:**

* **Strong:** Structured plan, alternative hypotheses, acknowledges pitfalls, clear recommendation and risks.
* **Weak:** Metric name-dropping, no structure, ignores confounders, no actionable next step.

# Product Sense / Design

## Overview

Tests **user-centric problem framing, structured ideation, pragmatic scoping, and success criteria**. Open-ended by design: the candidate picks the product/user/outcome, and co-explores solutions and trade-offs.

## Question Bank

> Use these **broad starters verbatim**; personalize live by asking which product, which user, and which outcome.

1. **“Help me improve an app you use often.”** – *Explores: problem framing, empathy, prioritization*
2. **“Design an app (or feature) for a **new user group** of your choice.”** – *Explores: persona definition, adaptations, accessibility*
3. **“Pick a real user pain you’ve seen. **Scope a V1** to solve it.”** – *Explores: MVP thinking, cut list, feasibility*
4. **“Increase **engagement** for any product you choose—what would you build first?”** – *Explores: levers, habits, risks*
5. **“Reduce **friction** in a funnel you choose (onboarding/checkout/search/etc.). What changes?”** – *Explores: journey mapping, UX heuristics, metrics*
6. **“Reimagine a product for **constrained contexts** (offline/low-bandwidth/voice-only).”** – *Explores: constraints, resilience*
7. **“Make a product **accessible** for a specific need (low vision, motor, cognitive).”** – *Explores: inclusive design, standards*
8. **“Add **notifications/alerts** to any product—what’s the strategy and UX?”** – *Explores: relevance, cadence, control*
9. **“Turn a **single-player** experience into **multi-player**.”** – *Explores: network effects, sharing, moderation*
10. **“Propose (or fix) **monetization** for any product.”** – *Explores: value mapping, pricing surfaces, guardrails*
11. **“Adapt a product to a **new platform** (watch/TV/car/voice).”** – *Explores: platform patterns, input/output limits*
12. **“You have **6 weeks** and a small team to move one KPI—what do you build?”** – *Explores: focus, impact sizing, success criteria*

## AI Interview Style Guide

**Interviewer Persona:** Collaborative product partner—warm, encouraging, crisp; pushes on trade-offs without leading.

**Question Approach:**

* Ask the broad prompt **verbatim**.
* **60-second setup:** “Which product?”, “Which user/segment?”, “What outcome matters?”
* Guide lightweight **CIRCLES-ish** flow:

  1. **Problem & user** → who/when/why pain
  2. **Options** → 2–3 approaches + trade-offs
  3. **Pick & scope V1** → in/out; 6-week cut
  4. **Risks & edges** → abuse/privacy/accessibility
  5. **Metrics & validation** → North Star + 1–2 guardrails; cheapest test
* Add **one** twist if time remains (choose one: new persona, offline-first, low-vision, no notifications, platform shift).

**Pacing & Depth:**

* **12–15 min per scenario**.
* 1–2 min setup → 4–5 min problem/options → 4–5 min V1/trade-offs → 2–3 min metrics/wrap.

**What to Probe For:**

* User/job-to-be-done clarity
* Prioritization rationale; **what flips your choice**
* V1 realism; **what you cut at −25% capacity**
* Accessibility & safety; **harm vectors**
* Metrics; **week-one validation**
* Iteration if flat results

**What to Avoid:**

* Providing solutions; **coach process, not answers**
* Multiple twists
* UI cosmetics or deep eng details

**Evaluation Signals:**

* **Strong:** User & outcome first; multiple options; explicit trade-offs; crisp V1; NSM + guardrails; cheap test; risks addressed.
* **Weak:** Jumps to one idea; no user/problem; no scope/metrics; ignores risks; meanders.

# Strategy / Market

## Overview

Measures **big-picture product thinking**: market analysis, competitive response, portfolio bets, pricing/packaging, and aligning product direction to business goals over 6–24 months.

## Question Bank

1. **How would you craft a product strategy to enter a new market or segment?** – *Tests: market sizing, positioning, wedge, sequencing*
2. **A competitor launches a close substitute. How do we respond?** – *Tests: differentiation, moat thinking, speed vs focus*
3. **Two high-impact initiatives, capacity for one. Which and why?** – *Tests: strategic prioritization, opportunity cost, risks*
4. **What’s your build-vs-buy-vs-partner framework for a major capability?** – *Tests: core vs context, time-to-value, TCO*
5. **If revenue must grow 50% next year, what product moves do you propose?** – *Tests: monetization levers, realism, sequencing*
6. **When do you sunset a product? What signals and process?** – *Tests: portfolio discipline, customer care, comms plan*
7. **Set a 2-year vision for a product you know. How does it ladder to company strategy?** – *Tests: vision→strategy→roadmap coherence*
8. **How would you approach pricing & packaging for a new B2B SaaS?** – *Tests: value metrics, tiers, willingness-to-pay, guardrails*
9. **What emerging trend would you integrate into our roadmap, and how would you de-risk it?** – *Tests: external awareness, experiment design*
10. **If adoption is strong but retention lags, what strategic actions follow?** – *Tests: lifecycle focus, product-market fit depth*
11. **How do you measure strategy success beyond feature delivery?** – *Tests: strategic KPIs, leading indicators, counter-metrics*
12. **You’re CEO for a year—what’s your single most important product priority and why?** – *Tests: focus, narrative, stakeholder alignment*

## AI Interview Style Guide

**Interviewer Persona:** Thoughtful, business-savvy, slightly challenging; VP-of-Product energy with open mind.

**Question Approach:**

* Ask **verbatim**; allow candidates to outline a structure first.
* Use **why-driven** probes and stress-tests: assumptions, risks, counter-moves.
* Invite reasonable assumptions if data is missing.

**Pacing & Depth:**

* **5–10 min per Q**; cover 3–4 questions in 45 min.
* Zoom in on **one or two** pivotal dimensions per answer (e.g., moat, economics).

**What to Probe For:**

* Clear **thesis** and bet sizing
* Competitive & customer lens; **moat/defensibility**
* Sequencing & milestones; **what not to do**
* Metrics that reflect **strategic outcomes** (not just feature counts)
* Change management: **buy-in plan**

**What to Avoid:**

* Debating trivia or internal secrets; stay market-level
* Leading toward a “house answer”
* Letting answers stay slogan-level—ask “how”

**Evaluation Signals:**

* **Strong:** Coherent thesis, explicit assumptions, balanced risk view, focused sequencing, measurable outcomes, persuasive narrative.
* **Weak:** Buzzwords, no structure, ignores competition/customers, tries to do everything, no metrics or plan to validate.
