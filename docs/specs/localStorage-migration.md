# localStorage Migration: User-Owned OpenAI Keys

**Status:** 📋 PLANNED  
**Created:** 2025-09-30  
**Updated:** 2025-09-30  
**Priority:** HIGH  
**Estimated effort:** 2-3 days

---

## Table of Contents

1. [Context & Motivation](#context--motivation)
2. [Goals & Non-Goals](#goals--non-goals)
3. [Current Architecture](#current-architecture)
4. [Target Architecture](#target-architecture)
5. [Deployment Model](#deployment-model)
6. [Data Structures](#data-structures)
7. [Implementation Plan](#implementation-plan)
8. [API Changes](#api-changes)
9. [UI Changes](#ui-changes)
10. [Migration Path](#migration-path)
11. [Testing Checklist](#testing-checklist)
12. [Open Questions](#open-questions)

---

## Context & Motivation

### Current State
- Backend SQLite database stores all interview data
- Server-side OpenAI API key (paid by maintainer)
- Vercel deployment uses ephemeral `/tmp` storage
- Data loss when serverless functions restart
- Complex deployment requirements
- Users must fork and deploy their own instance

### Problems
1. **Cost:** Maintainer pays for all users' OpenAI API usage (unsustainable)
2. **Scalability:** Database doesn't persist reliably on Vercel
3. **Complexity:** SQLite + serverless functions = deployment headaches
4. **Setup friction:** Users need Git knowledge, Vercel account, deployment skills
5. **Barrier to entry:** "Try it out" becomes "Learn to deploy first"

### Solution
- **Single hosted deployment** - Maintainer deploys once, everyone uses it
- **User-provided API keys** - Each user brings their own OpenAI key
- **localStorage for data** - All interviews/settings stored in user's browser
- **Zero setup required** - Just visit URL, add key, start interviewing
- **No hosting costs** - Static frontend + minimal proxy (if needed)
- **Optional forking** - Only for users who want to customize

---

## Goals & Non-Goals

### ✅ Goals

1. **Single hosted deployment:** Maintainer deploys once at a public URL (e.g., `interview-coach.vercel.app`)
2. **Zero setup for users:** Visit URL → Add API key → Start interviewing (no Git, no CLI, no deployment)
3. **Zero hosting costs:** Static frontend on free tier, no database, no compute costs
4. **User-owned API costs:** Each user provides their own OpenAI key and pays for their own usage
5. **User-owned data:** All interviews and settings stored locally in user's browser (localStorage)
6. **Privacy-first:** User data never leaves their browser, never stored on servers
7. **Offline-capable:** View past interviews without internet (once loaded)
8. **Export/Import:** Users can backup and restore their data as JSON files
9. **No data loss:** localStorage persists across sessions (until user clears browser data)
10. **Optional forking:** Advanced users can still fork for customization, but 99% won't need to

### ❌ Non-Goals

1. **Multi-device sync:** Not trying to sync across browsers/devices (users can export/import)
2. **Cloud backup:** Users responsible for exporting if they want backups
3. **Shared history:** Each browser has its own isolated history (privacy feature)
4. **User accounts:** No login, no authentication, purely client-side (simplicity feature)
5. **Team features:** Not supporting multiple users or collaboration
6. **Free OpenAI usage:** Users must have their own OpenAI API key (can't provide free AI)

---

## Current Architecture

```
┌─────────────┐
│   Frontend  │
│   (React)   │
└──────┬──────┘
       │
       │ HTTP API calls
       ▼
┌─────────────────────┐
│  Backend (Express)  │
│  - /api/interview/* │
│  - /api/categories  │
│  - Uses OPENAI_KEY  │
└──────┬──────────────┘
       │
       │ SQL queries
       ▼
┌─────────────────┐
│ SQLite Database │
│  interviews.db  │
│  (/tmp on Vercel)│
└─────────────────┘
       │
       │ Calls OpenAI API
       ▼
┌─────────────────┐
│  OpenAI API     │
│  (Maintainer's  │
│   account)      │
└─────────────────┘
```

### Current Data Flow

1. **Start interview:** Frontend → Backend → OpenAI Realtime API
2. **Collect transcript:** Frontend stores in React state
3. **End interview:** Frontend → Backend `/api/interview/summary`
4. **Generate summary:** Backend → OpenAI Chat API (with maintainer's key)
5. **Save data:** Backend → SQLite (`/tmp/interviews.db`)
6. **Load history:** Frontend → Backend → SQLite

---

## Target Architecture

```
┌──────────────────────┐
│      Frontend        │
│      (React)         │
│                      │
│  ┌────────────────┐  │
│  │  localStorage  │  │ ← All data stored here
│  │  - interviews  │  │
│  │  - settings    │  │
│  │  - user's key  │  │
│  └────────────────┘  │
└──────┬───────────────┘
       │
       │ (Optional) Proxy for CORS
       ▼
┌──────────────────────┐
│  Minimal Backend     │
│  (Stateless Proxy)   │  ← Optional, only if needed for CORS
│  - Forwards requests │
│  - Uses user's key   │
└──────┬───────────────┘
       │
       │ API calls with user's key
       ▼
┌──────────────────────┐
│    OpenAI API        │
│    (User's account)  │
└──────────────────────┘
```

### Target Data Flow

1. **One-time setup:** User enters their OpenAI API key → saved to localStorage
2. **Start interview:** Frontend → OpenAI Realtime API (direct, with user's key)
3. **Collect transcript:** Frontend stores in React state
4. **End interview:** Frontend → (Optional backend proxy) → OpenAI Chat API (with user's key)
5. **Save data:** Frontend → localStorage (JSON)
6. **Load history:** Frontend → localStorage (JSON)

---

## Deployment Model

### Primary Use Case: Single Hosted Instance

**Target user experience:**

```
User hears about the app
       ↓
Visits: https://interview-coach.vercel.app
       ↓
Sees welcome screen: "Get started in 2 minutes"
       ↓
Clicks "Get Started" → Modal opens
       ↓
Modal explains: "You'll need an OpenAI API key"
       ↓
User clicks "How to get a key" → Opens OpenAI docs
       ↓
User creates API key (5 minutes)
       ↓
User pastes key into app
       ↓
App validates key → ✓ Success
       ↓
User starts first interview immediately
```

**No Git. No CLI. No deployment. No configuration files. Just works.**

### How Multiple Users Share One Deployment

```
                    Maintainer deploys once
                              ↓
              https://interview-coach.vercel.app
                              ↓
              ┌───────────────┼───────────────┐
              ↓               ↓               ↓
         Browser A       Browser B       Browser C
              ↓               ↓               ↓
    ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
    │  localStorage   │ │  localStorage   │ │  localStorage   │
    │  - User A's key │ │  - User B's key │ │  - User C's key │
    │  - A's history  │ │  - B's history  │ │  - C's history  │
    └─────────────────┘ └─────────────────┘ └─────────────────┘
              ↓               ↓               ↓
         OpenAI API      OpenAI API      OpenAI API
       (A's account)   (B's account)   (C's account)
```

**Key insight:** localStorage is isolated per-browser, so each user has their own:
- API key (never shared)
- Interview history (private to their browser)
- Settings and preferences
- Custom categories

**Benefits:**
- ✅ Maintainer deploys once, everyone benefits
- ✅ Updates pushed to all users instantly
- ✅ No per-user infrastructure costs
- ✅ No user management/authentication needed
- ✅ Perfect for open-source projects

### Secondary Use Case: Fork for Customization

**For power users who want to modify the app:**

```
User wants custom features (e.g., different personas, branding)
       ↓
Fork the GitHub repo
       ↓
Clone locally: git clone [their-fork]
       ↓
Customize code (add personas, change styling, etc.)
       ↓
Deploy to their own Vercel: vercel deploy
       ↓
Share custom version with their team/friends
```

**Use cases for forking:**
- Company wants branded version for internal use
- Educator wants specialized question sets
- Researcher wants to collect anonymized data
- Developer wants to add experimental features

**Estimate:** < 1% of users will fork. 99% will use the hosted version.

### Deployment Comparison

| Aspect | Current (SQLite + Backend) | Target (localStorage + Hosted) |
|--------|----------------------------|-------------------------------|
| **User setup** | Fork → Clone → Deploy → Configure | Visit URL → Add key |
| **Time to first interview** | 30-60 minutes | 2-3 minutes |
| **Hosting cost** | $X/month per deployment | $0 (free tier) |
| **Data persistence** | Ephemeral on Vercel | Persists in browser |
| **Updates** | User must redeploy | Automatic for all users |
| **Privacy** | Data on server | Data never leaves browser |
| **Technical skill required** | Git, CLI, Vercel | None (just browser) |

---

## Data Structures

### localStorage Keys

```javascript
// Primary storage
'interview-history'     // Array of interview objects
'app-settings'          // User preferences (incl. API key)
'categories-custom'     // User's custom question categories

// Metadata
'app-version'           // For migration purposes
'last-export-date'      // Remind users to backup
```

### Interview Object Schema

```typescript
interface Interview {
  id: string;                    // UUID
  createdAt: number;             // Unix timestamp (ms)
  updatedAt: number;             // Unix timestamp (ms)
  
  // Session metadata
  difficulty: 'easy' | 'medium' | 'hard';
  persona: string;               // Persona name used
  resumeFilename?: string;       // If resume was uploaded
  jdSummary?: string;            // Job description summary
  
  // Interview content
  questions: Question[];         // List of questions asked
  transcript: Message[];         // Full conversation
  
  // Evaluation
  evaluation: {
    summary?: string;            // Raw AI response
    coaching?: {
      summary: string;
      strengths: string[];
      improvements: string[];
    };
  };
  
  // Metadata
  duration?: number;             // Interview length in seconds
  status: 'completed' | 'abandoned';
}

interface Question {
  id: string;
  text: string;
  category: string;
  difficulty: string;
}

interface Message {
  role: 'user' | 'assistant';
  text: string;
  timestamp: number;
}
```

### Settings Object Schema

```typescript
interface AppSettings {
  // API Configuration
  openaiApiKey: string;          // User's OpenAI API key (encrypted?)
  
  // Privacy & Storage
  autoExportReminder: boolean;   // Remind user to backup monthly
  lastExportDate?: number;       // Track when user last exported
  
  // UI Preferences
  theme?: 'light' | 'dark';      // Future: theme support
  defaultDifficulty?: string;    // Pre-select difficulty
  
  // Feature flags
  betaFeatures?: string[];       // Opt-in to experimental features
}
```

### Custom Categories Schema

```typescript
interface CustomCategory {
  id: string;
  name: string;
  description?: string;
  questions: Array<{
    id: string;
    text: string;
  }>;
  createdAt: number;
  updatedAt: number;
}
```

---

## Implementation Plan

### Phase 1: localStorage Service Layer

**Goal:** Create abstraction for all localStorage operations

**Tasks:**
1. Create `client/src/services/localStorage.js`
   - `getInterviews()` - Load all interviews
   - `getInterviewById(id)` - Load single interview
   - `saveInterview(interview)` - Save new or update existing
   - `deleteInterview(id)` - Remove interview
   - `exportInterviews()` - Generate JSON blob for download
   - `importInterviews(jsonBlob)` - Restore from backup
   - `getSettings()` - Load user settings
   - `saveSettings(settings)` - Update settings
   - `clearAllData()` - Nuclear option for testing

2. Add error handling
   - Quota exceeded detection
   - Corruption detection
   - Graceful fallbacks

3. Add versioning & migration
   - Track schema version
   - Auto-migrate old data formats

**Files to create:**
- `client/src/services/localStorage.js`
- `client/src/services/localStorageSchema.js` (TypeScript types/JSDoc)

**Estimated time:** 4-6 hours

---

### Phase 2: Settings UI for API Key

**Goal:** Let users input and manage their OpenAI API key

**Tasks:**
1. Create `client/src/components/settings/SettingsModal.jsx`
   - API key input field (password-style)
   - "Test Connection" button (validates key)
   - Key visibility toggle
   - Save/cancel actions
   - Link to OpenAI dashboard for getting keys

2. Add settings access
   - Gear icon in sidebar header
   - "Settings" button in prep wizard
   - First-time setup modal (blocks until key entered)

3. Key validation
   - Test key by making lightweight API call
   - Show success/error feedback
   - Detect invalid/expired keys

4. Security considerations
   - Warn user about keeping key secure
   - Offer to clear key on logout
   - Consider basic obfuscation (not encryption, localStorage isn't secure)

**Files to create:**
- `client/src/components/settings/SettingsModal.jsx`
- `client/src/components/settings/ApiKeyInput.jsx`
- `client/src/hooks/useSettings.js`

**Files to modify:**
- `client/src/components/Sidebar.jsx` (add settings button)
- `client/src/App.jsx` (add settings modal)

**Estimated time:** 4-6 hours

---

### Phase 3: Update Interview Storage

**Goal:** Replace all backend API calls for saving/loading interviews

**Tasks:**
1. Update `client/src/services/api.js`
   - Remove `saveInterview()` function
   - Remove `getInterviewHistory()` function
   - Remove `getInterviewById()` function
   - Keep `startInterviewSession()` (will update in Phase 4)
   - Keep `generateSummary()` (will update in Phase 4)

2. Update `client/src/hooks/useInterviewHistory.js`
   - Replace API calls with localStorage calls
   - `loadHistory()` → `localStorage.getInterviews()`
   - `loadInterviewDetail(id)` → `localStorage.getInterviewById(id)`
   - Keep same interface (components unchanged)

3. Update `client/src/App.jsx`
   - Replace `saveInterview` API call with `localStorage.saveInterview()`
   - Update after interview ends

4. Add optimistic updates
   - Save to localStorage immediately (no waiting for network)
   - Show instant feedback in sidebar

**Files to modify:**
- `client/src/services/api.js`
- `client/src/hooks/useInterviewHistory.js`
- `client/src/App.jsx`

**Files to remove:**
- N/A (backend removal comes later)

**Estimated time:** 3-4 hours

---

### Phase 4: Update OpenAI Integration

**Goal:** Use user's API key for all OpenAI calls

**Decision point:** Backend proxy vs. direct frontend calls

#### Option A: Direct Frontend Calls (Simpler)

**Tasks:**
1. Update `client/src/services/openai.js` (create new)
   - `createRealtimeSession(userKey, config)` - Start interview
   - `generateSummary(userKey, transcript)` - Get coaching feedback
   - Handle errors (quota, invalid key, rate limits)

2. Update interview start flow
   - Get user's API key from localStorage
   - Call OpenAI directly from frontend
   - Handle CORS (OpenAI should support it)

3. Update summary generation
   - Call OpenAI Chat API directly from frontend
   - Parse response into coaching structure
   - Save to localStorage

**Files to create:**
- `client/src/services/openai.js`

**Files to modify:**
- `client/src/App.jsx` (interview start/end logic)
- `client/src/hooks/useRealtimeInterview.js` (pass user key)

**Pros:** No backend needed at all, simplest deployment
**Cons:** API key visible in DevTools, CORS might be issue

#### Option B: Minimal Backend Proxy (Recommended)

**Tasks:**
1. Create minimal proxy endpoints
   - `api/realtime-session.js` - Proxy Realtime API session creation
   - `api/summarize.js` - Proxy Chat API for summary generation
   - Both accept user's API key in custom header (`X-OpenAI-Key`)
   - Stateless (no database, no storage)

2. Update frontend to send user's key
   - Add `X-OpenAI-Key` header to API calls
   - Keep using existing API service layer

3. Backend validation (optional)
   - Basic rate limiting per IP
   - Key format validation
   - Error handling

**Files to create:**
- `api/realtime-session.js` (simplified version of start-session.js)
- `api/summarize.js` (simplified version of summary.js)

**Files to modify:**
- `client/src/services/api.js` (add key to headers)

**Files to remove:**
- `api/interview/start-session.js` (replaced)
- `api/interview/summary.js` (replaced)

**Pros:** Handles CORS, hides key from DevTools, can add rate limiting
**Cons:** Still need to deploy backend

**Recommendation:** Go with Option B initially, can remove later if not needed

**Estimated time:** 4-6 hours

---

### Phase 5: Custom Categories Migration

**Goal:** Move custom categories to localStorage

**Tasks:**
1. Update `client/src/hooks/useCustomCategories.js` (create new)
   - Replace API calls with localStorage
   - `loadCategories()` → `localStorage.getCustomCategories()`
   - `saveCategory(category)` → `localStorage.saveCustomCategory()`
   - `deleteCategory(id)` → `localStorage.deleteCustomCategory()`

2. Update `client/src/components/prep/CustomCategoriesSection.jsx`
   - Use new hook instead of API calls
   - Instant feedback (no loading states needed)

3. Merge with default categories
   - Keep built-in categories in code
   - User categories loaded from localStorage
   - Combined list displayed in UI

**Files to create:**
- `client/src/hooks/useCustomCategories.js`

**Files to modify:**
- `client/src/components/prep/CustomCategoriesSection.jsx`
- `client/src/services/localStorage.js` (add category methods)

**Files to remove:**
- `api/categories.js` (after migration)
- `api/categories/[id].js` (after migration)

**Estimated time:** 2-3 hours

---

### Phase 6: Export/Import Functionality

**Goal:** Let users backup and restore their data

**Tasks:**
1. Create `client/src/components/settings/DataManagement.jsx`
   - "Export All Data" button
     - Downloads `interview-coach-backup-YYYY-MM-DD.json`
     - Includes interviews, settings (minus API key), categories
   - "Import Data" button
     - File picker for `.json` files
     - Preview before import
     - Merge or replace options
   - "Clear All Data" button
     - Confirmation dialog
     - Nuclear reset

2. Add to settings modal
   - "Data" tab in settings
   - Show storage usage (X MB used of 5-10 MB available)
   - Last export date
   - Reminder to export regularly

3. Export format
   ```json
   {
     "version": "1.0.0",
     "exportedAt": 1727654321000,
     "data": {
       "interviews": [...],
       "categories": [...],
       "settings": {
         // Exclude API key for security
       }
     }
   }
   ```

4. Import validation
   - Check version compatibility
   - Validate schema
   - Handle conflicts (duplicate IDs)
   - Show summary of what will be imported

**Files to create:**
- `client/src/components/settings/DataManagement.jsx`
- `client/src/utils/exportImport.js` (helper functions)

**Files to modify:**
- `client/src/components/settings/SettingsModal.jsx` (add Data tab)

**Estimated time:** 4-5 hours

---

### Phase 7: Backend Cleanup

**Goal:** Remove all unused backend code and dependencies

**Tasks:**
1. Remove SQLite dependencies
   - Delete `server/interviewStore.js`
   - Remove `better-sqlite3` from `package.json`
   - Remove `data/` directory

2. Remove unused API endpoints
   - `api/interview/save.js` ❌
   - `api/interview/history.js` ❌
   - `api/categories.js` ❌
   - `api/categories/[id].js` ❌
   - `api/interview/resume.js` (keep if still uploading to server)
   - `api/interview/jd.js` (keep if still uploading to server)

3. Simplify `vercel.json`
   - Remove database-related config
   - Remove complex routing rules
   - Minimal config for remaining endpoints

4. Update `README.md`
   - Remove database setup instructions
   - Remove Vercel-specific deployment notes
   - Add localStorage limitations section
   - Add backup recommendations

**Files to delete:**
- `server/interviewStore.js`
- `data/interviews.db*`
- `api/interview/save.js`
- `api/interview/history.js`
- `api/categories.js`
- `api/categories/[id].js`

**Files to modify:**
- `package.json` (remove better-sqlite3)
- `vercel.json` (simplify)
- `README.md` (update setup docs)
- `.gitignore` (remove data/ exclusions)

**Estimated time:** 2-3 hours

---

### Phase 8: Documentation & Polish

**Goal:** Update all docs for the hosted deployment model and add helpful UX improvements

**Tasks:**

1. **Rewrite `README.md` for hosted model**
   
   **New structure:**
   ```markdown
   # 🎤 Interview Coach
   
   Practice technical interviews with AI-powered feedback.
   
   ## 🚀 Quick Start (2 minutes)
   
   ### Option 1: Use the Hosted Version (Recommended)
   
   1. Visit **[interview-coach.vercel.app](https://interview-coach.vercel.app)**
   2. Click "Get Started"
   3. Get an OpenAI API key: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
   4. Paste your key and start interviewing!
   
   **Your API key and interviews are stored locally in your browser.**  
   No data is sent to our servers. You control your costs.
   
   **Cost:** ~$0.50-2.00 per interview (billed to your OpenAI account)
   
   ### Option 2: Fork & Customize
   
   Want to customize personas, branding, or features?
   
   1. Fork this repo
   2. Clone: `git clone your-fork`
   3. Install: `npm install && cd client && npm install`
   4. Deploy: `vercel deploy`
   
   ## 💾 Data & Privacy
   
   - **Local storage:** All interviews stored in your browser (localStorage)
   - **No server storage:** We never see your data or API key
   - **Backup:** Export your interviews anytime (Settings → Data → Export)
   - **Quota:** ~5-10 MB available (hundreds of interviews)
   
   ## 🔑 API Key Setup
   
   1. Visit [OpenAI API Keys](https://platform.openai.com/api-keys)
   2. Click "Create new secret key"
   3. Set a spending limit ($5-10/month recommended)
   4. Copy the key (starts with `sk-`)
   5. Paste into app when prompted
   
   ## 📊 Cost Breakdown
   
   Per interview (~15-30 minutes):
   - Realtime API: $0.40-1.50 (voice + processing)
   - Summary generation: $0.10-0.50 (GPT-4)
   - **Total:** $0.50-2.00
   
   **Tip:** Set spending limits on your OpenAI account to avoid surprises.
   
   ## 🛠️ Development (For Contributors)
   
   ```bash
   # Clone repo
   git clone https://github.com/your-username/interview-coach.git
   cd interview-coach
   
   # Install dependencies
   npm install
   cd client && npm install && cd ..
   
   # Run locally
   vercel dev  # Frontend at localhost:3000, API at /api
   ```
   
   ## 🤝 Contributing
   
   PRs welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.
   
   ## 📄 License
   
   MIT - see [LICENSE](LICENSE) for details.
   ```

2. **Create `docs/USER-GUIDE.md`**
   
   **Sections:**
   - Getting an OpenAI API key (step-by-step with screenshots)
   - Understanding costs (pricing breakdown)
   - Managing your data (export/import/backup)
   - Troubleshooting common issues
   - Privacy and security best practices

3. **Create `docs/FAQ.md`**
   
   **Common questions:**
   - "Do I need to create an account?" → No, just need OpenAI API key
   - "Where is my data stored?" → Locally in your browser
   - "How much does it cost?" → $0.50-2.00 per interview (you pay OpenAI)
   - "Is it secure?" → Yes, data never leaves your browser
   - "Can I use it offline?" → View past interviews yes, new interviews no
   - "How do I backup my data?" → Settings → Data → Export
   - "Can I share across devices?" → Export from one, import to another
   - "What if I lose my API key?" → Create new one in OpenAI dashboard

4. **Update `docs/workings.md`**
   - Remove backend/SQLite architecture sections
   - Add localStorage architecture
   - Update data flow diagrams to show hosted model
   - Emphasize single deployment, multiple users

5. **Update `DEPLOYMENT.md`**
   
   **Simplify to:**
   ```markdown
   # Deployment
   
   ## Hosted Version (Maintainer)
   
   Deploy once, everyone uses it:
   
   ```bash
   vercel deploy --prod
   ```
   
   That's it! Users just visit the URL and add their API keys.
   
   ## Custom Fork (Users)
   
   Want to customize? Fork and deploy your own:
   
   1. Fork repo on GitHub
   2. Connect to Vercel
   3. Deploy
   4. Share your custom URL
   
   No environment variables needed (users provide API keys).
   ```

6. **Add first-time user experience components**
   - `<FirstTimeSetup />` modal (as described in UI Changes)
   - Welcome message emphasizing simplicity
   - Clear call-to-action: "Get API Key"
   - Validation and helpful errors

7. **Add helpful UI hints throughout app**
   - Tooltip on API key field: "Get yours at platform.openai.com/api-keys"
   - Export reminder banner (monthly nudge)
   - Storage quota indicator (if > 80% full)
   - "Pro tip" messages for first-time users

**Files to create:**
- `docs/USER-GUIDE.md` - Comprehensive user documentation
- `docs/FAQ.md` - Common questions and answers
- `docs/CONTRIBUTING.md` - Guide for contributors (optional)

**Files to modify:**
- `README.md` - Complete rewrite for hosted model
- `docs/workings.md` - Remove backend, add localStorage
- `DEPLOYMENT.md` - Simplify to single-deploy model

**Estimated time:** 3-4 hours

---

## API Changes

### Endpoints to Keep (Option B: Backend Proxy)

```javascript
// POST /api/realtime-session
// Start a new Realtime API session
Request:
  Headers: { 'X-OpenAI-Key': 'sk-...' }
  Body: {
    persona: 'friendly_coach',
    difficulty: 'medium',
    questions: [...],
    resume?: string,
    jd?: string
  }
Response: {
  session: {
    client_secret: 'eph_...',
    expires_at: timestamp,
    model: 'gpt-4o-realtime-preview',
    instructions: '...'
  }
}

// POST /api/summarize
// Generate coaching summary
Request:
  Headers: { 'X-OpenAI-Key': 'sk-...' }
  Body: {
    transcript: [...],
    questions: [...],
    difficulty: 'medium'
  }
Response: {
  summary: '...',
  coaching: {
    summary: '...',
    strengths: [...],
    improvements: [...]
  }
}

// GET /api/health (optional, for monitoring)
Response: { status: 'ok' }
```

### Endpoints to Remove

```javascript
❌ POST   /api/interview/save
❌ GET    /api/interview/history
❌ GET    /api/interview/history/:id
❌ GET    /api/categories
❌ POST   /api/categories
❌ PATCH  /api/categories/:id
❌ DELETE /api/categories/:id
```

### File Upload Endpoints (TBD)

Decision needed: How to handle resume/JD uploads?

**Option 1:** Base64 encode and store in localStorage
- Pros: Simple, no backend
- Cons: Increases storage size significantly

**Option 2:** Keep upload endpoints, store in backend temporarily
- Pros: Doesn't bloat localStorage
- Cons: Still need backend storage (ephemeral /tmp is OK)

**Recommendation:** Option 1 initially, optimize later if needed

---

## UI Changes

### New Components (Priority Order)

#### 1. **First-Time Setup Modal** (`FirstTimeSetup.jsx`) - CRITICAL

**Purpose:** Onboard new users visiting the hosted site for the first time.

**Behavior:**
- Appears automatically if no API key in localStorage
- Blocks rest of app (modal backdrop, can't dismiss)
- Friendly, welcoming tone

**Content:**
```
🎤 Welcome to Interview Coach

This app helps you practice technical interviews with AI coaching.

To get started, you'll need an OpenAI API key:

1. Visit platform.openai.com/api-keys
2. Create a new API key
3. Paste it below

Your key is stored locally in your browser and never sent to our servers.

[Get an API Key →]  [I Already Have One →]

---

💡 Cost Estimate: ~$0.50-2.00 per interview
🔒 Your data stays in your browser
📤 Export/backup your interviews anytime
```

**Fields:**
- API key input (password-style, with show/hide toggle)
- "Test Connection" button (validates key)
- Help links: "What's an API key?" | "How much does it cost?" | "Is this secure?"

**Flow:**
1. User enters key
2. Clicks "Test Connection"
3. App makes lightweight API call (e.g., list models)
4. If valid: ✓ "Success! Starting your interview coach..." → modal closes
5. If invalid: ❌ "Invalid key. Please check and try again."

#### 2. **API Key Setup** (`ApiKeyInput.jsx`)

**Purpose:** Reusable input component for API key entry.

**Features:**
- Password-style input with visibility toggle (👁️ icon)
- Real-time validation (format check: starts with `sk-`)
- "Test Connection" button
- Link to OpenAI dashboard: "Get your key here →"
- Help text: "Your API key starts with 'sk-' and can be found at platform.openai.com/api-keys"

#### 3. **Settings Modal** (`SettingsModal.jsx`)

**Purpose:** Manage API key and app preferences after initial setup.

**Tabs:**
- **API Key** - View/update key, test connection, view usage
- **Data** - Export/import, storage usage, clear data
- **About** - App version, GitHub link, how to report issues

**Access:**
- Gear icon (⚙️) in sidebar header
- Link from prep wizard ("Settings")

#### 4. **Data Management Panel** (`DataManagement.jsx`)

**Purpose:** Let users backup and manage their data.

**Features:**
- Storage usage indicator: "Using 2.3 MB of ~5-10 MB available"
- "Export All Data" button → downloads `interview-coach-backup-2025-09-30.json`
- "Import Data" button → file picker, preview, merge/replace options
- "Clear All Data" button → confirmation dialog, nuclear reset
- Last export date: "Last backup: 2 weeks ago" (with ⚠️ if > 30 days)

#### 5. **Export Reminder Banner** (`ExportReminder.jsx`)

**Purpose:** Nudge users to backup regularly.

**Behavior:**
- Appears at top of app if last export > 30 days (or never exported and > 5 interviews)
- Soft yellow/blue banner (not alarming)
- "💾 Reminder: Back up your interviews! [Export Now] [Remind Me Later]"
- "Remind Later" = snooze for 7 days
- Dismisses on export

#### 6. **API Key Status Badge** (`ApiKeyStatus.jsx`)

**Purpose:** Show at-a-glance API key status.

**Display:**
- ✅ "Connected" (green) - Key valid, tested recently
- ⚠️ "Not Set" (yellow) - No key in localStorage
- ❌ "Invalid" (red) - Key test failed (expired/revoked)

**Placement:**
- Prep wizard sidebar (top of settings card)
- Settings modal (API Key tab)

### Modified Components

#### 1. **App.jsx**

**Changes:**
- Add `<FirstTimeSetup />` modal (renders if no API key)
- Add `<SettingsModal />` (controlled by state)
- Add `<ExportReminder />` banner (conditional)
- Remove backend polling/loading states
- Simplify: localStorage is synchronous, no async loading

#### 2. **Sidebar.jsx**

**Changes:**
- Add ⚙️ gear icon button → opens settings modal
- Remove "Loading history..." spinner (instant from localStorage)
- Add storage quota warning (if > 80% full): "⚠️ Storage almost full - export old interviews"
- Update "No interviews yet" empty state: "Start your first interview to see it here!"

#### 3. **PrepWizard.jsx**

**Changes:**
- Add API Key Status badge to sidebar: `<ApiKeyStatus />`
- Link to settings: "Need to update your API key? [Settings]"
- Show usage estimate: "Est. cost: $0.75-1.50 per interview"

---

### First-Time User Journey (Hosted)

```
New user visits https://interview-coach.vercel.app
       ↓
App checks localStorage for API key → Not found
       ↓
<FirstTimeSetup /> modal appears (fullscreen, can't dismiss)
       ↓
User reads welcome message, clicks "Get an API Key"
       ↓
Opens platform.openai.com/api-keys in new tab
       ↓
User creates key (takes ~2 minutes)
       ↓
Returns to app, pastes key into modal
       ↓
Clicks "Test Connection" → App validates key
       ↓
✓ Success! Modal fades out, prep wizard appears
       ↓
Sidebar shows: ✅ "Connected" | "New Interview" button ready
       ↓
User configures first interview and starts!
```

**Total time:** 2-5 minutes (depending on OpenAI signup)

---

## Migration Path

### For Users Migrating from Current Version

Since current Vercel deployment loses data anyway, we don't need a formal migration!

**Simple approach:**
1. User updates to new version
2. Sees first-time setup (enter API key)
3. Starts fresh with localStorage
4. Previous interviews were ephemeral anyway (lost on Vercel restart)

**Optional: Manual Migration Script**

If we want to be nice, provide a one-time export from SQLite:

```javascript
// scripts/export-from-sqlite.js
// Run locally before upgrading
const Database = require('better-sqlite3');
const db = new Database('./data/interviews.db');

const interviews = db.prepare('SELECT * FROM interviews').all();
const categories = db.prepare('SELECT * FROM user_categories').all();

const exportData = {
  version: '1.0.0',
  exportedAt: Date.now(),
  data: { interviews, categories }
};

fs.writeFileSync('migration-export.json', JSON.stringify(exportData, null, 2));
console.log('Exported to migration-export.json - import this in new version!');
```

User can then import this file in the new version.

---

## Testing Checklist

### localStorage Service
- [ ] Save interview to localStorage
- [ ] Load interview from localStorage
- [ ] Update existing interview
- [ ] Delete interview
- [ ] Handle quota exceeded gracefully
- [ ] Handle corrupted data gracefully
- [ ] Version migration works

### Settings & API Key
- [ ] Save API key to localStorage
- [ ] Load API key on app start
- [ ] Test key validation (valid key)
- [ ] Test key validation (invalid key)
- [ ] Test key validation (expired key)
- [ ] Key hidden by default (password field)
- [ ] Key visibility toggle works
- [ ] First-time setup blocks app until key entered

### Interview Flow
- [ ] Start interview with user's API key
- [ ] Realtime API connection established
- [ ] Transcript collected during interview
- [ ] End interview generates summary with user's key
- [ ] Interview saved to localStorage immediately
- [ ] Interview appears in sidebar instantly
- [ ] Load interview detail from sidebar

### Custom Categories
- [ ] Create custom category (saved to localStorage)
- [ ] Edit custom category
- [ ] Delete custom category
- [ ] Categories persist across sessions
- [ ] Custom categories appear in question selection

### Export/Import
- [ ] Export all data downloads JSON file
- [ ] Exported file has correct structure
- [ ] Import file picker accepts JSON
- [ ] Import preview shows what will be imported
- [ ] Import merge combines with existing data
- [ ] Import replace clears existing data first
- [ ] Import validates schema
- [ ] Import rejects invalid files

### Error Handling
- [ ] Invalid API key shows helpful error
- [ ] Rate limit exceeded shows helpful error
- [ ] Quota exceeded (localStorage) warns user
- [ ] Network errors handled gracefully
- [ ] Corrupted localStorage data detected and cleared

### Cross-Browser Testing
- [ ] Chrome (localStorage + Realtime API)
- [ ] Firefox (localStorage + Realtime API)
- [ ] Safari (localStorage + Realtime API)
- [ ] Mobile Chrome (responsive + localStorage)
- [ ] Mobile Safari (responsive + localStorage)

### Edge Cases
- [ ] User clears localStorage (first-time setup appears)
- [ ] User switches browsers (independent history)
- [ ] User has 50+ interviews (performance OK)
- [ ] User imports duplicate interview IDs (handled)
- [ ] User exports with no data (empty but valid file)

---

## Open Questions

### 1. Resume/JD File Handling

**Question:** How should we store uploaded resume/JD files?

**Options:**
- A) Base64 encode and store in localStorage (simple but bloated)
- B) Keep backend upload endpoints, store in `/tmp` temporarily (need backend)
- C) Read files directly in browser, never upload (privacy win, limits features)

**Recommendation:** Option C for now, can add upload later if needed

---

### 2. Backend: Keep or Remove?

**Question:** Should we keep a minimal backend proxy or go 100% frontend?

**Options:**
- A) 100% frontend: Call OpenAI directly, no backend at all
- B) Minimal proxy: Forward requests with user's key, handle CORS
- C) Hybrid: Optional backend for advanced features, works without it

**Recommendation:** Start with B (minimal proxy), test if CORS is an issue, move to A if not needed

---

### 3. API Key Storage

**Question:** How should we store the user's API key in localStorage?

**Options:**
- A) Plain text (localStorage isn't secure anyway)
- B) Simple obfuscation (security theater, but looks better)
- C) Encrypt with user-provided passphrase (adds friction)

**Recommendation:** Option A with big warning that localStorage isn't secure. Educate user to use API key with spending limits.

---

### 4. Storage Quota

**Question:** What should we do when user approaches localStorage quota?

**Options:**
- A) Warn at 80%, block saves at 100%
- B) Auto-delete oldest interviews when quota reached
- C) Prompt user to export and archive old interviews

**Recommendation:** Option A + C: Warn user, suggest export, but don't auto-delete without permission.

---

### 5. Multi-Device Sync

**Question:** Should we add optional cloud sync?

**Options:**
- A) No, keep it simple (localStorage only)
- B) Optional: Let users sync via their own backend (advanced)
- C) Add built-in sync service (complex, defeats purpose)

**Recommendation:** Option A for v1, document how users can self-host sync if desired.

---

### 6. Migration from Current Version

**Question:** Should we help users migrate existing interviews?

**Options:**
- A) No migration needed (Vercel data is ephemeral anyway)
- B) Provide manual export script (users run locally)
- C) Add one-time auto-migration (complex, rarely used)

**Recommendation:** Option B: Provide `scripts/export-from-sqlite.js` for users who want to preserve local dev data.

---

## Success Metrics

**How we'll know this was successful:**

### User Experience Metrics

1. **Time to first interview:** < 5 minutes (from landing page to "Start Interview")
2. **Setup friction:** Zero technical knowledge required (no Git, no CLI, no deployment)
3. **Bounce rate:** < 30% of users who land on the page should complete first interview
4. **Return rate:** Users return to practice multiple times (localStorage enables this)

### Technical Metrics

5. **Hosting cost:** $0/month for maintainer (Vercel free tier sufficient)
6. **API costs:** $0/month for maintainer (users pay their own)
7. **Reliability:** Zero data loss incidents (localStorage > ephemeral /tmp)
8. **Performance:** App loads in < 2 seconds, history loads instantly (no backend queries)

### Community Metrics

9. **Adoption:** More users (no deployment barrier = wider audience)
10. **Contributions:** More contributors (simpler architecture = easier to understand)
11. **Support load:** Fewer "how do I deploy?" issues (99% use hosted version)
12. **Forking rate:** < 5% (most users don't need to fork)

---

## Risks & Mitigations

| Risk | Impact | Likelihood | Mitigation |
|------|--------|-----------|------------|
| Users don't backup data, lose history | Medium | High | Monthly export reminder, auto-download on significant milestones |
| CORS blocks direct OpenAI calls | Medium | Low | Keep minimal backend proxy as fallback |
| localStorage quota exceeded | Low | Low | Warn at 80%, suggest export/archive |
| Users expose API keys | High | Medium | Clear warnings, link to spending limits setup |
| Browser compatibility issues | Medium | Low | Test across major browsers, provide fallbacks |

---

## Timeline Estimate

**Total effort:** 2-3 days (for experienced developer)

- **Day 1:** Phases 1-3 (localStorage service, settings UI, interview storage)
- **Day 2:** Phases 4-5 (OpenAI integration, categories migration)  
- **Day 3:** Phases 6-8 (export/import, cleanup, docs)

**Parallelization opportunities:**
- Settings UI (Phase 2) can be done alongside localStorage service (Phase 1)
- Export/Import (Phase 6) can be done alongside custom categories (Phase 5)

---

## Follow-Up Tasks (Post-Implementation)

1. **Performance monitoring:** Track localStorage usage over time
2. **User feedback:** Survey about new setup experience
3. **Documentation video:** Record setup walkthrough
4. **Cost calculator:** Tool to estimate OpenAI API costs per interview
5. **Optional features:**
   - Theme support (light/dark mode)
   - Advanced export options (CSV, PDF)
   - Interview templates (save common configurations)
   - Statistics dashboard (total interviews, average scores, etc.)

---

## Key Takeaways

### Why This Architecture is Perfect for This Project

**The core insight:** By combining user-provided API keys with localStorage, we eliminate ALL traditional deployment friction:

```
Traditional SaaS Model:
- Maintainer pays for hosting ❌
- Maintainer pays for AI API usage ❌
- Need database infrastructure ❌
- Users must create accounts ❌
- Complex deployment ❌

Our Model:
- Maintainer deploys once to free tier ✅
- Users pay for their own AI usage ✅
- No database needed ✅
- No user accounts needed ✅
- Zero configuration deployment ✅
```

### Value Propositions by Stakeholder

**For the Maintainer:**
- 🎉 Deploy once, serve unlimited users
- 💰 Zero ongoing costs (no database, no compute, no AI usage)
- 🚀 Push updates instantly to all users
- 📉 Minimal support burden (no "how do I deploy?" questions)
- 🏗️ Simpler architecture = easier to maintain

**For End Users:**
- ⚡ Instant access (no signup, no waiting)
- 🔒 Complete privacy (data never leaves browser)
- 💵 Control costs (set OpenAI spending limits)
- 📦 Own data (export anytime)
- 🎮 Just works (no technical skills needed)

**For Contributors:**
- 📖 Simpler codebase (no backend complexity)
- 🧪 Easier testing (localStorage > database)
- 🔧 Faster development (no deployment for testing)
- 🤝 Lower barrier to contribute

### This Enables Network Effects

The hosted model creates a flywheel:

```
More users → More feedback → Better product → More users
     ↑                                            ↓
More stars ← More contributors ← Simpler code ← Wider reach
```

**Without deployment friction:**
- Someone tweets about it → 1,000 people try it (not 10)
- Blog post mentions it → Instant demo available
- Hackathon uses it → No setup time wasted
- Company wants to evaluate → Decision makers can try it themselves

### The "Just Visit a URL" Advantage

Compare user acquisition funnels:

**Current (Fork Required):**
```
100 people interested
  ↓ 80% bounce (too much work)
20 fork the repo
  ↓ 50% fail deployment
10 get it working
  ↓ 50% abandon after one try
5 active users
```

**With Hosted Model:**
```
100 people interested
  ↓ 20% bounce (need OpenAI key)
80 add API key
  ↓ 10% have issues
70 complete first interview
  ↓ 30% continue using
20+ active users (4x improvement!)
```

### Long-Term Sustainability

**This architecture scales to thousands of users without any changes:**

- No database to scale
- No backend to scale  
- No costs that increase with users
- No infrastructure to maintain

**Even if the project becomes "popular,"** the maintainer's costs remain $0 and the architecture works identically.

### Perfect for Open Source

This model is **ideal for side projects and open source:**

✅ No ongoing financial commitment  
✅ No risk of surprise bills  
✅ Can "set it and forget it"  
✅ Encourages experimentation (users can try freely)  
✅ Lowers barrier for contributions  
✅ Makes project sustainable long-term  

---

## References

- [Web Storage API (localStorage)](https://developer.mozilla.org/en-US/docs/Web/API/Web_Storage_API)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [OpenAI Realtime API Guide](https://platform.openai.com/docs/guides/realtime)
- [Best Practices for localStorage](https://developer.mozilla.org/en-US/docs/Web/API/Web_Storage_API/Using_the_Web_Storage_API)
- [Quota Management API](https://developer.mozilla.org/en-US/docs/Web/API/StorageManager/estimate)

---

## Notes

- This spec assumes OpenAI Realtime API supports direct browser connections (it does via ephemeral keys)
- localStorage is synchronous (may block main thread for large datasets) - monitor performance
- Consider IndexedDB for future if storage needs grow significantly
- Remember to handle Safari's stricter localStorage policies (private browsing mode)

---

**Last updated:** 2025-09-30  
**Author:** AI Assistant  
**Reviewers:** TBD

